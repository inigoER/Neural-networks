{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_final_IA166831.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Final project AI\n",
        "#Iñigo Echeagaray Rodríguez\n",
        "#May 9th 2022\n",
        "Copyright TensorFlow 2019\n",
        "\n",
        "Special thanks to Dr. Gerardo Ayala San Martin\n",
        "\n",
        "In this project, a dataset will be analyzed by Iñigo Echeagaray using TensorFlow, the dataset contains data for 7 different kinds of dried beans.\n",
        "\n",
        "The dataset citation is given as: KOKLU, M. and OZKAN, I.A., (2020), Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques. Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507.\n",
        "\n",
        "In the given citation, one can visualize the original study, in which they also analize the dataset using matlab GUI with a multi layer perceptron, support vector machine, k nearest neighbors and decision trees, they also compare the different performances.\n",
        "\n",
        "In this project, the dataset with the image features (not the images themselves) will be analyzed, features will be selected to achieve a good classification metric with a neural network.\n",
        "\n",
        "The goal is to reach a model with accuracy that is at least as good as the ones given in the study."
      ],
      "metadata": {
        "id": "u21cDQu5fVrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up and library importation"
      ],
      "metadata": {
        "id": "M40goHs7fyU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary complements to read excel files\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aUpMkUIcs11",
        "outputId": "de70aa4e-568d-4801-d44f-1305a161064f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YHL93slafVO3"
      },
      "outputs": [],
      "source": [
        "#Importing Tensorflow, to create the neural network\n",
        "import tensorflow as TensorFlow\n",
        "#Importing numpy, to perform algebraic operations on arrays and the dataset\n",
        "import numpy as np\n",
        "#Importing pandas, to be able to manipulate and read the excel dataset\n",
        "import pandas as pd\n",
        "#Importing model_selection, to randomly split our dataset into trainig and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import a preprocessing part of sklearn that has an encoder to convert our string labels to integers (class 0,1,2,3,4,5 and 6)\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the dataset to the environment\n",
        "Data=pd.read_excel(\"DriedBeansDataset.xlsx\")\n",
        "#Check if there are any null values on the dataset, and see which features it has\n",
        "Data.isnull().sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5oGSFKddM0r",
        "outputId": "d8654d27-c8ae-41cb-9366-9cc26024970f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Area               0\n",
              "Perimeter          0\n",
              "MajorAxisLength    0\n",
              "MinorAxisLength    0\n",
              "AspectRation       0\n",
              "Eccentricity       0\n",
              "ConvexArea         0\n",
              "EquivDiameter      0\n",
              "Extent             0\n",
              "Solidity           0\n",
              "roundness          0\n",
              "Compactness        0\n",
              "ShapeFactor1       0\n",
              "ShapeFactor2       0\n",
              "ShapeFactor3       0\n",
              "ShapeFactor4       0\n",
              "Class              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luckily, there are no null values in the dataset, now, we can split the dataset into two, one dataset containing the features and the other the classes."
      ],
      "metadata": {
        "id": "kl7-o31HgKiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset of features, dropping the class columns\n",
        "DataFeat=Data.drop('Class', axis=1)\n",
        "#Dataset(array) of the class\n",
        "DataClass=Data[['Class']]"
      ],
      "metadata": {
        "id": "_HLjpgs7lq7W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since tensorflow needs numerical classes (that is, class 0,1,2,3,4,5,6), we need to turn the strings in DataClass to numbers, based on the name."
      ],
      "metadata": {
        "id": "A7YzWqHeJm6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the numerical transformer\n",
        "ClassTransformerToInt = preprocessing.LabelEncoder()\n",
        "#Fit the classes to the transformer (assign each class a number from 0 to 6)\n",
        "ClassTransformerToInt.fit(DataClass.Class)\n",
        "#Create array that has the numerical classes\n",
        "DataClassNum=ClassTransformerToInt.transform(DataClass.Class)\n",
        "#Visualize unique values (the classes) of the array\n",
        "np.unique(DataClassNum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgy0dtwPJziQ",
        "outputId": "1d51045a-9994-414a-89a1-9dfeeae34b35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection"
      ],
      "metadata": {
        "id": "-K5Buyfei6Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should scale the features so that they all have a range of 0 to 1, in order to simplify the model and also to make it easier to select features, one way to do this is by using the formula for each feature value X: (X-min)/(max-min), this is so that the min value of the set becomes 0, and the max value of the set becomes 1, with every other value in between.\n",
        "\n",
        "We can make this operation for every value of the feature columns using the apply function."
      ],
      "metadata": {
        "id": "K9rhhstvwnRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply this to every column of the dataset of features\n",
        "for col in DataFeat:\n",
        "  #Save the max value of the column\n",
        "  max=DataFeat[col].max()\n",
        "  #Save the min value of the column\n",
        "  min=DataFeat[col].min()\n",
        "  #Make sure that the column has different values, that is, that its minimum value isn't the same as its maximum value\n",
        "  if max!=min:\n",
        "    #Make the feature column equal to the scaled feature column, the apply function is a function that goes through a whole array (in this case, column) and\n",
        "    #applies a mathematical formula to each value x\n",
        "    DataFeat[col]=DataFeat[col].apply(\n",
        "        lambda value: (value-min)/(max-min)\n",
        "        )\n",
        "  #If the column is a column of all equal values, drop it, since it doesn't provide any info\n",
        "  else:\n",
        "    #Here, the argument axis=1 implies that the dropping will be made for a column, not a row (which would be axis=0, the default)\n",
        "    DataFeat.drop(col,axis=1)"
      ],
      "metadata": {
        "id": "TGxV2-1CwnRv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the scaled Dataset of features\n",
        "DataFeat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "dloSfJIleZl6",
        "outputId": "3ed705b4-7987-4348-e4a0-39bb0e2f1aa0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
              "0      0.034053   0.058574         0.044262         0.152142      0.122612   \n",
              "1      0.035500   0.077557         0.030479         0.178337      0.051577   \n",
              "2      0.038259   0.068035         0.052633         0.158190      0.131521   \n",
              "3      0.040940   0.082942         0.048548         0.177691      0.091623   \n",
              "4      0.041504   0.065313         0.032862         0.200679      0.025565   \n",
              "...         ...        ...              ...              ...           ...   \n",
              "13606  0.092559   0.160862         0.189318         0.187843      0.375584   \n",
              "13607  0.092576   0.159358         0.176450         0.201964      0.321303   \n",
              "13608  0.092739   0.160605         0.176384         0.203370      0.318558   \n",
              "13609  0.092773   0.163657         0.179703         0.200669      0.330472   \n",
              "13610  0.092824   0.169448         0.200882         0.176768      0.423337   \n",
              "\n",
              "       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
              "0          0.477797    0.033107       0.070804  0.671024  0.922824   0.934823   \n",
              "1          0.278472    0.034991       0.073577  0.735504  0.871514   0.793138   \n",
              "2          0.496448    0.037126       0.078816  0.716671  0.932141   0.914511   \n",
              "3          0.403864    0.041389       0.083854  0.731365  0.761614   0.826871   \n",
              "4          0.165680    0.040123       0.084906  0.700538  0.949832   0.988408   \n",
              "...             ...         ...            ...       ...       ...        ...   \n",
              "13606      0.788553    0.089967       0.172180  0.512286  0.942381   0.852151   \n",
              "13607      0.746241    0.089910       0.172207  0.786890  0.947954   0.862952   \n",
              "13608      0.743877    0.090219       0.172463  0.561689  0.936648   0.855785   \n",
              "13609      0.753971    0.090623       0.172517  0.482741  0.908991   0.834795   \n",
              "13610      0.819877    0.090347       0.172598  0.751569  0.933322   0.795826   \n",
              "\n",
              "       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \n",
              "0         0.786733      0.593432      0.833049      0.750996      0.980620  \n",
              "1         0.903549      0.547447      0.967315      0.884987      0.974979  \n",
              "2         0.773514      0.582016      0.800942      0.736200      0.987196  \n",
              "3         0.829912      0.552408      0.854744      0.799846      0.893675  \n",
              "4         0.951583      0.510741      1.000000      0.941770      0.989116  \n",
              "...            ...           ...           ...           ...           ...  \n",
              "13606     0.465175      0.531785      0.382135      0.412185      0.974113  \n",
              "13607     0.523974      0.509582      0.426233      0.470848      0.970912  \n",
              "13608     0.525351      0.508683      0.427019      0.472240      0.943025  \n",
              "13609     0.510145      0.514216      0.415330      0.456919      0.913342  \n",
              "13610     0.416526      0.550320      0.346892      0.364762      0.970162  \n",
              "\n",
              "[13611 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6a0aac5-26f2-4bf5-b868-5cafd3f9ee70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Solidity</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "      <th>ShapeFactor4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.034053</td>\n",
              "      <td>0.058574</td>\n",
              "      <td>0.044262</td>\n",
              "      <td>0.152142</td>\n",
              "      <td>0.122612</td>\n",
              "      <td>0.477797</td>\n",
              "      <td>0.033107</td>\n",
              "      <td>0.070804</td>\n",
              "      <td>0.671024</td>\n",
              "      <td>0.922824</td>\n",
              "      <td>0.934823</td>\n",
              "      <td>0.786733</td>\n",
              "      <td>0.593432</td>\n",
              "      <td>0.833049</td>\n",
              "      <td>0.750996</td>\n",
              "      <td>0.980620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.077557</td>\n",
              "      <td>0.030479</td>\n",
              "      <td>0.178337</td>\n",
              "      <td>0.051577</td>\n",
              "      <td>0.278472</td>\n",
              "      <td>0.034991</td>\n",
              "      <td>0.073577</td>\n",
              "      <td>0.735504</td>\n",
              "      <td>0.871514</td>\n",
              "      <td>0.793138</td>\n",
              "      <td>0.903549</td>\n",
              "      <td>0.547447</td>\n",
              "      <td>0.967315</td>\n",
              "      <td>0.884987</td>\n",
              "      <td>0.974979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.038259</td>\n",
              "      <td>0.068035</td>\n",
              "      <td>0.052633</td>\n",
              "      <td>0.158190</td>\n",
              "      <td>0.131521</td>\n",
              "      <td>0.496448</td>\n",
              "      <td>0.037126</td>\n",
              "      <td>0.078816</td>\n",
              "      <td>0.716671</td>\n",
              "      <td>0.932141</td>\n",
              "      <td>0.914511</td>\n",
              "      <td>0.773514</td>\n",
              "      <td>0.582016</td>\n",
              "      <td>0.800942</td>\n",
              "      <td>0.736200</td>\n",
              "      <td>0.987196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.040940</td>\n",
              "      <td>0.082942</td>\n",
              "      <td>0.048548</td>\n",
              "      <td>0.177691</td>\n",
              "      <td>0.091623</td>\n",
              "      <td>0.403864</td>\n",
              "      <td>0.041389</td>\n",
              "      <td>0.083854</td>\n",
              "      <td>0.731365</td>\n",
              "      <td>0.761614</td>\n",
              "      <td>0.826871</td>\n",
              "      <td>0.829912</td>\n",
              "      <td>0.552408</td>\n",
              "      <td>0.854744</td>\n",
              "      <td>0.799846</td>\n",
              "      <td>0.893675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.041504</td>\n",
              "      <td>0.065313</td>\n",
              "      <td>0.032862</td>\n",
              "      <td>0.200679</td>\n",
              "      <td>0.025565</td>\n",
              "      <td>0.165680</td>\n",
              "      <td>0.040123</td>\n",
              "      <td>0.084906</td>\n",
              "      <td>0.700538</td>\n",
              "      <td>0.949832</td>\n",
              "      <td>0.988408</td>\n",
              "      <td>0.951583</td>\n",
              "      <td>0.510741</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941770</td>\n",
              "      <td>0.989116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13606</th>\n",
              "      <td>0.092559</td>\n",
              "      <td>0.160862</td>\n",
              "      <td>0.189318</td>\n",
              "      <td>0.187843</td>\n",
              "      <td>0.375584</td>\n",
              "      <td>0.788553</td>\n",
              "      <td>0.089967</td>\n",
              "      <td>0.172180</td>\n",
              "      <td>0.512286</td>\n",
              "      <td>0.942381</td>\n",
              "      <td>0.852151</td>\n",
              "      <td>0.465175</td>\n",
              "      <td>0.531785</td>\n",
              "      <td>0.382135</td>\n",
              "      <td>0.412185</td>\n",
              "      <td>0.974113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13607</th>\n",
              "      <td>0.092576</td>\n",
              "      <td>0.159358</td>\n",
              "      <td>0.176450</td>\n",
              "      <td>0.201964</td>\n",
              "      <td>0.321303</td>\n",
              "      <td>0.746241</td>\n",
              "      <td>0.089910</td>\n",
              "      <td>0.172207</td>\n",
              "      <td>0.786890</td>\n",
              "      <td>0.947954</td>\n",
              "      <td>0.862952</td>\n",
              "      <td>0.523974</td>\n",
              "      <td>0.509582</td>\n",
              "      <td>0.426233</td>\n",
              "      <td>0.470848</td>\n",
              "      <td>0.970912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13608</th>\n",
              "      <td>0.092739</td>\n",
              "      <td>0.160605</td>\n",
              "      <td>0.176384</td>\n",
              "      <td>0.203370</td>\n",
              "      <td>0.318558</td>\n",
              "      <td>0.743877</td>\n",
              "      <td>0.090219</td>\n",
              "      <td>0.172463</td>\n",
              "      <td>0.561689</td>\n",
              "      <td>0.936648</td>\n",
              "      <td>0.855785</td>\n",
              "      <td>0.525351</td>\n",
              "      <td>0.508683</td>\n",
              "      <td>0.427019</td>\n",
              "      <td>0.472240</td>\n",
              "      <td>0.943025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13609</th>\n",
              "      <td>0.092773</td>\n",
              "      <td>0.163657</td>\n",
              "      <td>0.179703</td>\n",
              "      <td>0.200669</td>\n",
              "      <td>0.330472</td>\n",
              "      <td>0.753971</td>\n",
              "      <td>0.090623</td>\n",
              "      <td>0.172517</td>\n",
              "      <td>0.482741</td>\n",
              "      <td>0.908991</td>\n",
              "      <td>0.834795</td>\n",
              "      <td>0.510145</td>\n",
              "      <td>0.514216</td>\n",
              "      <td>0.415330</td>\n",
              "      <td>0.456919</td>\n",
              "      <td>0.913342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13610</th>\n",
              "      <td>0.092824</td>\n",
              "      <td>0.169448</td>\n",
              "      <td>0.200882</td>\n",
              "      <td>0.176768</td>\n",
              "      <td>0.423337</td>\n",
              "      <td>0.819877</td>\n",
              "      <td>0.090347</td>\n",
              "      <td>0.172598</td>\n",
              "      <td>0.751569</td>\n",
              "      <td>0.933322</td>\n",
              "      <td>0.795826</td>\n",
              "      <td>0.416526</td>\n",
              "      <td>0.550320</td>\n",
              "      <td>0.346892</td>\n",
              "      <td>0.364762</td>\n",
              "      <td>0.970162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13611 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6a0aac5-26f2-4bf5-b868-5cafd3f9ee70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6a0aac5-26f2-4bf5-b868-5cafd3f9ee70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6a0aac5-26f2-4bf5-b868-5cafd3f9ee70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the dataset has a lot features, because a lot of features can be extracted from images.\n",
        "\n",
        "When creating a model, having a lot of features can be a good thing, but it can also lead to overfitting or an increased difficulty (running time) when fitting the model, it can also sometimes add unnecessary noise that makes the model worse, so there are techniques to select the best features to use.\n",
        "\n",
        "We can see that this can be related to searching algorithms, we could create an algorithm that implements the models with different features and chooses the best ones, however, there are a lot of features and therefore that would be a lot of models, 16 features can make 2^16 models (if we include the model that uses no features), testing all of them would not be ideal (breadth first is not a good idea) and we need a good solution (depth first is not a good idea), and even informed algorithms would not be a very good idea, since even testing just a few may take a long time (the algorithm would have to train and test the model each time, even testing a few, this would take a long time), so we need to find a way to select the features beforehand.\n",
        "\n",
        "One way we can do this is selecting the features that have the highest scaled standard deviation (scaled so that the standard deviations are actually comparable)**, why? We can remember that the standard deviation is a measure of variability in a set, a feature with high standard deviation is a feature that contains several different values among the dataset, features with low standard deviation are features that have very similar values across the dataset, this implies that they \"don't have much to say\", meaning, if the feature has almost the same value along the dataset, we cannot use it to distinguish between classes, because the different values for different classes will be very close to eachother.\n",
        "\n",
        "Thankfully, since we already scaled the features, we can simply check the standard deviation of each feature using the std() function.\n",
        "\n",
        "**Note: If the features weren't scaled, the standard deviation wouldn't tell us much, for example, is a standard deviation of \"3\", a high standard deviation? It depends on the set range, right? Meaning, if the set has a range of 0 to 6, the standard deviation is extremely high, but if the set has a range of 0 to 1,000,000, the standard deviation is extremely low and 1,000,000 is almost certainly an isolated outlier, that is why when comparing the standard deviations between features, the feature values should be scaled so they all have the same range, in this case, from 0 to 1."
      ],
      "metadata": {
        "id": "kXjYF9WxeY3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the standard deviation of the features\n",
        "DataFeat.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivH8YNlRnBug",
        "outputId": "d9a8dccd-8d16-49a7-ea64-88972d5cf1b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Area               0.125212\n",
              "Perimeter          0.146710\n",
              "MajorAxisLength    0.154332\n",
              "MinorAxisLength    0.133171\n",
              "AspectRation       0.175517\n",
              "Eccentricity       0.132860\n",
              "ConvexArea         0.122744\n",
              "EquivDiameter      0.144996\n",
              "Extent             0.157895\n",
              "Solidity           0.061783\n",
              "roundness          0.118786\n",
              "Compactness        0.177989\n",
              "ShapeFactor1       0.147006\n",
              "ShapeFactor2       0.192168\n",
              "ShapeFactor3       0.175392\n",
              "ShapeFactor4       0.083898\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that some features have lower standard deviations compared to the others, like ShapeFactor4 and Solidity, we can try to remove features that have a standard deviation of under 0.1, or under 10% of the maximum value, which is 1.\n",
        "\n",
        "Note: This value is chosen because this is an already \"clean\" dataset, so we want to be conservative when removing features, in a \"dirty\" dataset, one should use a larger threshold."
      ],
      "metadata": {
        "id": "9dHmAIJA1z4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the array to save the names of columns that will be used\n",
        "column_array=[]\n",
        "#Apply this to every column of the dataset of scaled features\n",
        "for col in DataFeat:\n",
        "  #Obtain the Standard Deviation of the feature column\n",
        "  FeatureStandardDeviation = DataFeat[col].std()\n",
        "  #If the Standard Deviation of the feature column is over 0.1\n",
        "  if FeatureStandardDeviation >= 0.1:\n",
        "     #save the feature column name to the column array\n",
        "    column_array.append(col)\n",
        "#Display the columns to be used\n",
        "column_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTkrCBx02JKn",
        "outputId": "4a99c817-aa48-4ba3-af6a-7addb919be76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Area',\n",
              " 'Perimeter',\n",
              " 'MajorAxisLength',\n",
              " 'MinorAxisLength',\n",
              " 'AspectRation',\n",
              " 'Eccentricity',\n",
              " 'ConvexArea',\n",
              " 'EquivDiameter',\n",
              " 'Extent',\n",
              " 'roundness',\n",
              " 'Compactness',\n",
              " 'ShapeFactor1',\n",
              " 'ShapeFactor2',\n",
              " 'ShapeFactor3']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new feature dataframe that contains only the features we are going to use\n",
        "DataFeatAnalysis=DataFeat[column_array]\n",
        "#Display the new dataframe\n",
        "DataFeatAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "NTZhprb93X2M",
        "outputId": "0a9c170c-8e97-4fc0-88e1-49a511a6e534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
              "0      0.034053   0.058574         0.044262         0.152142      0.122612   \n",
              "1      0.035500   0.077557         0.030479         0.178337      0.051577   \n",
              "2      0.038259   0.068035         0.052633         0.158190      0.131521   \n",
              "3      0.040940   0.082942         0.048548         0.177691      0.091623   \n",
              "4      0.041504   0.065313         0.032862         0.200679      0.025565   \n",
              "...         ...        ...              ...              ...           ...   \n",
              "13606  0.092559   0.160862         0.189318         0.187843      0.375584   \n",
              "13607  0.092576   0.159358         0.176450         0.201964      0.321303   \n",
              "13608  0.092739   0.160605         0.176384         0.203370      0.318558   \n",
              "13609  0.092773   0.163657         0.179703         0.200669      0.330472   \n",
              "13610  0.092824   0.169448         0.200882         0.176768      0.423337   \n",
              "\n",
              "       Eccentricity  ConvexArea  EquivDiameter    Extent  roundness  \\\n",
              "0          0.477797    0.033107       0.070804  0.671024   0.934823   \n",
              "1          0.278472    0.034991       0.073577  0.735504   0.793138   \n",
              "2          0.496448    0.037126       0.078816  0.716671   0.914511   \n",
              "3          0.403864    0.041389       0.083854  0.731365   0.826871   \n",
              "4          0.165680    0.040123       0.084906  0.700538   0.988408   \n",
              "...             ...         ...            ...       ...        ...   \n",
              "13606      0.788553    0.089967       0.172180  0.512286   0.852151   \n",
              "13607      0.746241    0.089910       0.172207  0.786890   0.862952   \n",
              "13608      0.743877    0.090219       0.172463  0.561689   0.855785   \n",
              "13609      0.753971    0.090623       0.172517  0.482741   0.834795   \n",
              "13610      0.819877    0.090347       0.172598  0.751569   0.795826   \n",
              "\n",
              "       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  \n",
              "0         0.786733      0.593432      0.833049      0.750996  \n",
              "1         0.903549      0.547447      0.967315      0.884987  \n",
              "2         0.773514      0.582016      0.800942      0.736200  \n",
              "3         0.829912      0.552408      0.854744      0.799846  \n",
              "4         0.951583      0.510741      1.000000      0.941770  \n",
              "...            ...           ...           ...           ...  \n",
              "13606     0.465175      0.531785      0.382135      0.412185  \n",
              "13607     0.523974      0.509582      0.426233      0.470848  \n",
              "13608     0.525351      0.508683      0.427019      0.472240  \n",
              "13609     0.510145      0.514216      0.415330      0.456919  \n",
              "13610     0.416526      0.550320      0.346892      0.364762  \n",
              "\n",
              "[13611 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc973ac7-b9a2-489e-ad6a-f313565bc332\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.034053</td>\n",
              "      <td>0.058574</td>\n",
              "      <td>0.044262</td>\n",
              "      <td>0.152142</td>\n",
              "      <td>0.122612</td>\n",
              "      <td>0.477797</td>\n",
              "      <td>0.033107</td>\n",
              "      <td>0.070804</td>\n",
              "      <td>0.671024</td>\n",
              "      <td>0.934823</td>\n",
              "      <td>0.786733</td>\n",
              "      <td>0.593432</td>\n",
              "      <td>0.833049</td>\n",
              "      <td>0.750996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.077557</td>\n",
              "      <td>0.030479</td>\n",
              "      <td>0.178337</td>\n",
              "      <td>0.051577</td>\n",
              "      <td>0.278472</td>\n",
              "      <td>0.034991</td>\n",
              "      <td>0.073577</td>\n",
              "      <td>0.735504</td>\n",
              "      <td>0.793138</td>\n",
              "      <td>0.903549</td>\n",
              "      <td>0.547447</td>\n",
              "      <td>0.967315</td>\n",
              "      <td>0.884987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.038259</td>\n",
              "      <td>0.068035</td>\n",
              "      <td>0.052633</td>\n",
              "      <td>0.158190</td>\n",
              "      <td>0.131521</td>\n",
              "      <td>0.496448</td>\n",
              "      <td>0.037126</td>\n",
              "      <td>0.078816</td>\n",
              "      <td>0.716671</td>\n",
              "      <td>0.914511</td>\n",
              "      <td>0.773514</td>\n",
              "      <td>0.582016</td>\n",
              "      <td>0.800942</td>\n",
              "      <td>0.736200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.040940</td>\n",
              "      <td>0.082942</td>\n",
              "      <td>0.048548</td>\n",
              "      <td>0.177691</td>\n",
              "      <td>0.091623</td>\n",
              "      <td>0.403864</td>\n",
              "      <td>0.041389</td>\n",
              "      <td>0.083854</td>\n",
              "      <td>0.731365</td>\n",
              "      <td>0.826871</td>\n",
              "      <td>0.829912</td>\n",
              "      <td>0.552408</td>\n",
              "      <td>0.854744</td>\n",
              "      <td>0.799846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.041504</td>\n",
              "      <td>0.065313</td>\n",
              "      <td>0.032862</td>\n",
              "      <td>0.200679</td>\n",
              "      <td>0.025565</td>\n",
              "      <td>0.165680</td>\n",
              "      <td>0.040123</td>\n",
              "      <td>0.084906</td>\n",
              "      <td>0.700538</td>\n",
              "      <td>0.988408</td>\n",
              "      <td>0.951583</td>\n",
              "      <td>0.510741</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.941770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13606</th>\n",
              "      <td>0.092559</td>\n",
              "      <td>0.160862</td>\n",
              "      <td>0.189318</td>\n",
              "      <td>0.187843</td>\n",
              "      <td>0.375584</td>\n",
              "      <td>0.788553</td>\n",
              "      <td>0.089967</td>\n",
              "      <td>0.172180</td>\n",
              "      <td>0.512286</td>\n",
              "      <td>0.852151</td>\n",
              "      <td>0.465175</td>\n",
              "      <td>0.531785</td>\n",
              "      <td>0.382135</td>\n",
              "      <td>0.412185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13607</th>\n",
              "      <td>0.092576</td>\n",
              "      <td>0.159358</td>\n",
              "      <td>0.176450</td>\n",
              "      <td>0.201964</td>\n",
              "      <td>0.321303</td>\n",
              "      <td>0.746241</td>\n",
              "      <td>0.089910</td>\n",
              "      <td>0.172207</td>\n",
              "      <td>0.786890</td>\n",
              "      <td>0.862952</td>\n",
              "      <td>0.523974</td>\n",
              "      <td>0.509582</td>\n",
              "      <td>0.426233</td>\n",
              "      <td>0.470848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13608</th>\n",
              "      <td>0.092739</td>\n",
              "      <td>0.160605</td>\n",
              "      <td>0.176384</td>\n",
              "      <td>0.203370</td>\n",
              "      <td>0.318558</td>\n",
              "      <td>0.743877</td>\n",
              "      <td>0.090219</td>\n",
              "      <td>0.172463</td>\n",
              "      <td>0.561689</td>\n",
              "      <td>0.855785</td>\n",
              "      <td>0.525351</td>\n",
              "      <td>0.508683</td>\n",
              "      <td>0.427019</td>\n",
              "      <td>0.472240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13609</th>\n",
              "      <td>0.092773</td>\n",
              "      <td>0.163657</td>\n",
              "      <td>0.179703</td>\n",
              "      <td>0.200669</td>\n",
              "      <td>0.330472</td>\n",
              "      <td>0.753971</td>\n",
              "      <td>0.090623</td>\n",
              "      <td>0.172517</td>\n",
              "      <td>0.482741</td>\n",
              "      <td>0.834795</td>\n",
              "      <td>0.510145</td>\n",
              "      <td>0.514216</td>\n",
              "      <td>0.415330</td>\n",
              "      <td>0.456919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13610</th>\n",
              "      <td>0.092824</td>\n",
              "      <td>0.169448</td>\n",
              "      <td>0.200882</td>\n",
              "      <td>0.176768</td>\n",
              "      <td>0.423337</td>\n",
              "      <td>0.819877</td>\n",
              "      <td>0.090347</td>\n",
              "      <td>0.172598</td>\n",
              "      <td>0.751569</td>\n",
              "      <td>0.795826</td>\n",
              "      <td>0.416526</td>\n",
              "      <td>0.550320</td>\n",
              "      <td>0.346892</td>\n",
              "      <td>0.364762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13611 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc973ac7-b9a2-489e-ad6a-f313565bc332')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc973ac7-b9a2-489e-ad6a-f313565bc332 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc973ac7-b9a2-489e-ad6a-f313565bc332');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model creation"
      ],
      "metadata": {
        "id": "p55ry45T5G12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and testing datasets"
      ],
      "metadata": {
        "id": "KXSseR5U8BRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have chosen which features we are going to use, we can start creating the model, first off, we need a training and testing dataset"
      ],
      "metadata": {
        "id": "YMaRtZrK5Lb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the training and testing sets, in order, these are:\n",
        "#The feature part of the training set, the feature part of the testing set, the class part of the training set and the class part of the testing set\n",
        "#Test_size=0.1 implies that, out of the 100% that is the whole data set, 90% will be dedicated to training and 10% to testing\n",
        "DataFeatAnalysis_train, DataFeatAnalysis_test, DataClass_train, DataClass_test = train_test_split(DataFeatAnalysis, DataClassNum, test_size=0.1)\n",
        "#Important note: in smaller datasets, one should limit the test size to avoid overfitting, in our case, because our dataset is relatively large, \n",
        "#we can get away with bigger train sizes"
      ],
      "metadata": {
        "id": "X0MKt-U-5KJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to convert the dataframes to numpy arrays."
      ],
      "metadata": {
        "id": "uFKZMpUW_ckX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the dataframes to numpy arrays\n",
        "FeaturesArray_train=DataFeatAnalysis_train.to_numpy()\n",
        "FeaturesArray_test=DataFeatAnalysis_test.to_numpy()\n",
        "#Change the name of the Class arrays to fit the context\n",
        "ClassArray_train=DataClass_train\n",
        "ClassArray_test=DataClass_test"
      ],
      "metadata": {
        "id": "u2jow6aL_tAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify that the shape of each set is correct (14 for the features, none for the class):\n",
        "print('Shape of training Feature array: ' + str(FeaturesArray_train.shape))\n",
        "print('Shape of training Class array: ' + str(ClassArray_train.shape))\n",
        "print('Shape of testing Feature array:  '  + str(FeaturesArray_test.shape))\n",
        "print('Shape of testing Class array:  '  + str(ClassArray_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PflTPwFG69ep",
        "outputId": "1e6b5871-dc92-4393-e596-6a8ad4561d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training Feature array: (12249, 14)\n",
            "Shape of training Class array: (12249,)\n",
            "Shape of testing Feature array:  (1362, 14)\n",
            "Shape of testing Class array:  (1362,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss and base model creation"
      ],
      "metadata": {
        "id": "O9UxJJ2e8FO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets recall that to create the keras sequential model, we stack layers with one input tensor and one output tensor.\n",
        "\n",
        "The Flatten layer flattens the input, in our case it is not necessary since our input is already a one dimensional array.\n",
        "\n",
        "The Dense layer is the densely connected layer of the Neural Network.\n",
        "\n",
        "The dimension of the output is set to 16.\n",
        "\n",
        "The rectifier or ReLU (Rectified Linear Unit) activation function is an activation function defined as the positive part of:\n",
        "\n",
        "f(x)= max(0,x)\n",
        "\n",
        "where x is the input to a neuron. This is also known as a ramp function.\n",
        "\n",
        "The Dropout layer randomly sets input units to 0 with a certain rate (0.2 in this case) at each step during training time, this makes it so the network isn't too dependent on just some neurons, which would cause overfitting.\n",
        "\n",
        "In our case, we use 3 dense layers to appropriately train the network (1 input layer, 1 hidden layer and 1 output layer), no flatten layer (our input is already one dimensional), and 2 dropout layers to minimize overfitting because of our large train size."
      ],
      "metadata": {
        "id": "_a7uLskn8cXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the neural model\n",
        "NeuralModel= TensorFlow.keras.models.Sequential([\n",
        "  #The output has a shape of 66, the activation function will be relu, and the input has shape 14,none\n",
        "  TensorFlow.keras.layers.Dense(66,activation='relu',input_shape=(14,)),\n",
        "  #Set the rate for the dropout to 0.2\n",
        "  TensorFlow.keras.layers.Dropout(0.2),\n",
        "  #Add another dense layer with output of shape 22\n",
        "  TensorFlow.keras.layers.Dense(22),\n",
        "  #Set the rate for the dropout to 0.2\n",
        "  TensorFlow.keras.layers.Dropout(0.2),\n",
        "  #Final dense layer, with the 7 classes as its output\n",
        "  TensorFlow.keras.layers.Dense(7)\n",
        "])\n",
        "\n",
        "print(NeuralModel.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvmtKFp87nI6",
        "outputId": "1283c99a-43d6-49db-c372-77964f3b6d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 66)                990       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 66)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 22)                1474      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 22)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 7)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,625\n",
            "Trainable params: 2,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create the logit vector to start creating the loss function"
      ],
      "metadata": {
        "id": "Hxi3uv-9OD0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the vector of predictions for the neural model for just \"one row\" of the FeaturesArray\n",
        "predictions = NeuralModel(FeaturesArray_train[:1])\n",
        "#Display the vector\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLM2OtF68KCU",
        "outputId": "40178e05-d562-4f0d-e0d6-bbc84d7ed490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.29375508 -0.3100034  -0.2588174  -0.22384077 -0.05209648 -0.47950733\n",
            "   0.08307511]], shape=(1, 7), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the loss function using SparseCategoricalCrossentropy, since there are multiple classes, it takes a vector of logits with a True index and returns the loss for each example."
      ],
      "metadata": {
        "id": "VR7eaWcHO39s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the loss function\n",
        "lossFunc = TensorFlow.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#Compute the loss of the first example\n",
        "print(lossFunc(ClassArray_train[:1], predictions).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOJ6agQjBSQ5",
        "outputId": "782702a7-6ac9-45ff-a551-e0832830defc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0351434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our function is appropriate, since it is close the -ln(1/7)~=1.94, which is the expected loss for an untrained model of 7 classes (it just chooses 1 of the 7 classes pretty much randomly)."
      ],
      "metadata": {
        "id": "9wK5I_ykPXV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final model creation"
      ],
      "metadata": {
        "id": "Qg7kAynHP6ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the final model, we need to use a compile method, in this case, we can use ADAM, which is an optimized gradient descent method, among the different methods, ADAM is of the ones that converge faster (reaches high accuracy in few epochs), and it can also reach very high accuracy."
      ],
      "metadata": {
        "id": "ndChZonhQCFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the adam optimizer with our loss function and measure its accuracy\n",
        "NeuralModel.compile(optimizer='adam',\n",
        "              loss=lossFunc,\n",
        "              metrics=['Accuracy'])"
      ],
      "metadata": {
        "id": "fTBRbMRPUbqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: 80 epochs were chosen after some trial and error, progress seems to stall at that number of epochs."
      ],
      "metadata": {
        "id": "MBUFV7TKMU4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model using the feature array, for 80 epochs or repetitions\n",
        "NeuralModel.fit(FeaturesArray_train, ClassArray_train, epochs=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXZnG5BhUm2q",
        "outputId": "8a4b1843-3f50-48a6-c637-8e3b2e6724ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "383/383 [==============================] - 2s 2ms/step - loss: 1.0422 - Accuracy: 0.6201\n",
            "Epoch 2/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.4797 - Accuracy: 0.8335\n",
            "Epoch 3/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.4130 - Accuracy: 0.8521\n",
            "Epoch 4/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3764 - Accuracy: 0.8686\n",
            "Epoch 5/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3615 - Accuracy: 0.8693\n",
            "Epoch 6/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3477 - Accuracy: 0.8764\n",
            "Epoch 7/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3354 - Accuracy: 0.8807\n",
            "Epoch 8/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3212 - Accuracy: 0.8842\n",
            "Epoch 9/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3202 - Accuracy: 0.8868\n",
            "Epoch 10/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3155 - Accuracy: 0.8907\n",
            "Epoch 11/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3061 - Accuracy: 0.8922\n",
            "Epoch 12/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3007 - Accuracy: 0.8968\n",
            "Epoch 13/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.3028 - Accuracy: 0.8950\n",
            "Epoch 14/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2978 - Accuracy: 0.8967\n",
            "Epoch 15/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2934 - Accuracy: 0.8962\n",
            "Epoch 16/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2942 - Accuracy: 0.8975\n",
            "Epoch 17/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2920 - Accuracy: 0.8957\n",
            "Epoch 18/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2890 - Accuracy: 0.8993\n",
            "Epoch 19/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2895 - Accuracy: 0.8986\n",
            "Epoch 20/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2854 - Accuracy: 0.9004\n",
            "Epoch 21/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2904 - Accuracy: 0.8986\n",
            "Epoch 22/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2814 - Accuracy: 0.8997\n",
            "Epoch 23/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2823 - Accuracy: 0.9013\n",
            "Epoch 24/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2797 - Accuracy: 0.9015\n",
            "Epoch 25/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2771 - Accuracy: 0.9034\n",
            "Epoch 26/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2744 - Accuracy: 0.9050\n",
            "Epoch 27/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2780 - Accuracy: 0.9020\n",
            "Epoch 28/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2796 - Accuracy: 0.9032\n",
            "Epoch 29/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2750 - Accuracy: 0.9033\n",
            "Epoch 30/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2771 - Accuracy: 0.9060\n",
            "Epoch 31/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2758 - Accuracy: 0.9033\n",
            "Epoch 32/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2760 - Accuracy: 0.9030\n",
            "Epoch 33/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2750 - Accuracy: 0.9037\n",
            "Epoch 34/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2739 - Accuracy: 0.9037\n",
            "Epoch 35/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2713 - Accuracy: 0.9081\n",
            "Epoch 36/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2716 - Accuracy: 0.9047\n",
            "Epoch 37/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2740 - Accuracy: 0.9050\n",
            "Epoch 38/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2699 - Accuracy: 0.9033\n",
            "Epoch 39/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2718 - Accuracy: 0.9029\n",
            "Epoch 40/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2674 - Accuracy: 0.9068\n",
            "Epoch 41/80\n",
            "383/383 [==============================] - 1s 3ms/step - loss: 0.2647 - Accuracy: 0.9074\n",
            "Epoch 42/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2642 - Accuracy: 0.9052\n",
            "Epoch 43/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2644 - Accuracy: 0.9065\n",
            "Epoch 44/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2664 - Accuracy: 0.9068\n",
            "Epoch 45/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2683 - Accuracy: 0.9053\n",
            "Epoch 46/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2652 - Accuracy: 0.9083\n",
            "Epoch 47/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2651 - Accuracy: 0.9064\n",
            "Epoch 48/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2654 - Accuracy: 0.9070\n",
            "Epoch 49/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2711 - Accuracy: 0.9067\n",
            "Epoch 50/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2655 - Accuracy: 0.9070\n",
            "Epoch 51/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2655 - Accuracy: 0.9053\n",
            "Epoch 52/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2636 - Accuracy: 0.9064\n",
            "Epoch 53/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2636 - Accuracy: 0.9057\n",
            "Epoch 54/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2630 - Accuracy: 0.9073\n",
            "Epoch 55/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2636 - Accuracy: 0.9078\n",
            "Epoch 56/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2612 - Accuracy: 0.9096\n",
            "Epoch 57/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2633 - Accuracy: 0.9082\n",
            "Epoch 58/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2595 - Accuracy: 0.9105\n",
            "Epoch 59/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2623 - Accuracy: 0.9070\n",
            "Epoch 60/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2640 - Accuracy: 0.9079\n",
            "Epoch 61/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2594 - Accuracy: 0.9084\n",
            "Epoch 62/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2596 - Accuracy: 0.9089\n",
            "Epoch 63/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2620 - Accuracy: 0.9068\n",
            "Epoch 64/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2584 - Accuracy: 0.9076\n",
            "Epoch 65/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2584 - Accuracy: 0.9117\n",
            "Epoch 66/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2572 - Accuracy: 0.9108\n",
            "Epoch 67/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2562 - Accuracy: 0.9113\n",
            "Epoch 68/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2563 - Accuracy: 0.9122\n",
            "Epoch 69/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2580 - Accuracy: 0.9091\n",
            "Epoch 70/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2639 - Accuracy: 0.9103\n",
            "Epoch 71/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2597 - Accuracy: 0.9120\n",
            "Epoch 72/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2546 - Accuracy: 0.9091\n",
            "Epoch 73/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2576 - Accuracy: 0.9110\n",
            "Epoch 74/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2591 - Accuracy: 0.9082\n",
            "Epoch 75/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2580 - Accuracy: 0.9089\n",
            "Epoch 76/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2570 - Accuracy: 0.9090\n",
            "Epoch 77/80\n",
            "383/383 [==============================] - 1s 3ms/step - loss: 0.2595 - Accuracy: 0.9109\n",
            "Epoch 78/80\n",
            "383/383 [==============================] - 1s 3ms/step - loss: 0.2576 - Accuracy: 0.9094\n",
            "Epoch 79/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2548 - Accuracy: 0.9098\n",
            "Epoch 80/80\n",
            "383/383 [==============================] - 1s 2ms/step - loss: 0.2557 - Accuracy: 0.9096\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9b591fa5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our model reached an accuracy of around 91%, which isn't too good, but it isn't bad either, compared to the accuracies of the original models from the study, it is a little on the low side but nothing too bad, now, we need to check how accurate it is on unseen data, to make sure it wasn't overfit."
      ],
      "metadata": {
        "id": "MDNKrNdwGx30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model to verify how good it is on previously unseen data\n",
        "NeuralModel.evaluate(FeaturesArray_test,  ClassArray_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIHlShEXWd68",
        "outputId": "5478634c-c70c-4814-aca5-312be9b7b0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 - 0s - loss: 0.2081 - Accuracy: 0.9258 - 217ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20805798470973969, 0.9258443713188171]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is a little better on previously unseen data, with an accuracy of around 92.5%, which is pretty decent, all things considered, I wasn't able to get better results than the models described in the study, but it is still a pretty good result."
      ],
      "metadata": {
        "id": "VE2gazZcHJNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that we have the model, save it:\n",
        "NeuralModel.save(\"NeuralNetwork166831.h5\")"
      ],
      "metadata": {
        "id": "h266MUAaHtZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
